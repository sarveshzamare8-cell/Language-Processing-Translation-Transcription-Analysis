# Comparative Analysis and Enhancement of Machine Translation and Transcription Systems

**Duration:** Sep 2023 â€“ Jun 2024 (9 months)  
**Author:** Sarvesh Zamare â€” [GitHub/sarveshzamare8-cell](https://github.com/sarveshzamare8-cell)

---

## TL;DR
This repository contains reproducible experiments comparing rule-based, statistical, and neural approaches for Machine Translation (MT) and Automatic Speech Recognition (ASR). I fine-tuned pre-trained models and measured improvements using BLEU for MT and WER for ASR. The project demonstrated large improvements in contextual translation accuracy through domain-specific fine-tuning.

---
## ðŸ“ Repository Structure
Language-Processing-Translation-Transcription-Analysis/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Unprocessed datasets (text and audio)
â”‚ â”‚ â”œâ”€â”€ english_texts.csv
â”‚ â”‚ â”œâ”€â”€ hindi_texts.csv
â”‚ â”‚ â””â”€â”€ speech_samples.wav
â”‚ â””â”€â”€ processed/ # Cleaned and aligned datasets
â”‚
â”œâ”€â”€ models/ # Pre-trained and fine-tuned models
â”‚ â”œâ”€â”€ asr_model.h5
â”‚ â””â”€â”€ model_mt.pt
â”‚
â”œâ”€â”€ notebooks/ # Jupyter notebooks for experiments
â”‚ â”œâ”€â”€ translation_experiments.ipynb
â”‚ â””â”€â”€ asr_comparison.ipynb
â”‚
â”œâ”€â”€ scripts/ # Python scripts for preprocessing and evaluation
â”‚ â”œâ”€â”€ preprocessing.py
â”‚ â”œâ”€â”€ translation_pipeline.py
â”‚ â””â”€â”€ asr_evaluation.py
â”‚
â”œâ”€â”€ reports/ # Results and documentation
â”‚ â”œâ”€â”€ bleu_vs_epoch.png
â”‚ â”œâ”€â”€ wer_results.csv
â”‚ â””â”€â”€ final_report.pdf
â”‚
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

---
## Project summary
**Goal:** Evaluate and improve translation and transcription pipelines â€” from rule-based and statistical baselines to modern neural approaches â€” with a focus on contextual accuracy and real-world usability.

**Contributions**
- Implemented baseline systems (rule-based & statistical) and neural baselines (Hugging Face transformers, Wav2Vec).
- Fine-tuned pre-trained translation and ASR models on domain-specific data.
- Built evaluation scripts that compute BLEU (sacrebleu) and WER (jiwer).
- Packaged reproducible notebooks and a small demo scaffold.

---

## Key results (summary)
> Replace these numbers with your final metrics in `reports/evaluation_results.pdf`.

- **BLEU**: baseline â†’ **improved by ~20â€“25%** after fine-tuning.  
- **WER**: reduced by **X%** using fine-tuned Wav2Vec models.  
See `/reports/evaluation_results.pdf` for full tables and plots.

---

## Repo structure
